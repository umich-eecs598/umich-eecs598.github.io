---
layout: project
parent: Reports
title: "DOPE-Plus: Enhancements in Feature Extraction and Data Generation for 6D Pose Estimation"
description: Group 6 final project report for DeepRob at the University of Michigan.
authors:
  - name: Jeffery Chen
    # social: "https://topipari.com"
    affiliation: University of Michigan
  - name: Yuqiao Luo
    # social: "https://xiaoxiaodu.net/"
    affiliation: University of Michigan
  - name: Longzhen Yuan
    affiliation: University of Michigan
---


<!-- This shows how to add an image (or gif) in markdown -->
<div class="center-image">
<img alt="Teaser Figure" src="{{ site.baseurl }}/assets/projects/reports/DOPE-Plus/ViT-DOPE_structure.png" />
</div>


<div class="project-links" markdown="1">
[![]({{ site.baseurl }}/assets/logos/acrobat.svg){: .text-logo } Report]({{ site.baseurl }}/assets/projects/reports/DOPE-Plus/ROB599_Final_report.pdf){: .btn .btn-grey .mr-6 }
[![]({{ site.baseurl }}/assets/logos/github-mark.svg){: .text-logo } Dataset Code](https://github.com/swtyree/hope-dataset){: .btn .btn-grey target="_blank" rel="noopener noreferrer" .mr-6}
[![]({{ site.baseurl }}/assets/logos/github-mark.svg){: .text-logo } Model Code](https://github.com/jypipi/Our_DOPE){: .btn .btn-grey target="_blank" rel="noopener noreferrer" }
</div>


## Abstract

This essay investigated the possibility of improving DOPE's network architecture. We proposed to replace the original VGG19 convolutional neural network with vision transformer as our feature extractor. Then we trained our modified model with HOPE dataset and our own generated domain randomized data, to see if this architecture change could bring a better performance. The experiment result shows that ViT does perform better than VGG19. This shows a promising future to further improve DOPE by using different feature extractors.  

## Introduction

As robotics are becoming more promising in the future society, people are exploring ways to give robotics the ability to handle daily affairs. Many of these affairs require basic operations such as fetching, which is based on an accurate pose estimation of the target objects. This article investigated the DOPE (Deep Object Pose Estimation) proposed by J. Tremblay et al. in 2018, and tried to make extensions based on the original network structure. The original work used a VGG19 as a feature extractor, while in our work, we modified that to Vision Transformer(ViT), due to its better ability of overall feature extraction, especially for relationships of multi-objects. After that, we applied the synthetic data method (proposed by original DOPE paper) to a new dataset and plugged it into our network. Our goal is to advance the accuracy of 3D objects' pose estimation and verify the effectiveness of image synthesis method for the purpose of manipulating the robot arm to operate these objects in real world.  

DOPE  

DOPE (Deep Object Pose Estimation) is a one-shot, deep neural network-based system designed to estimate the 3D poses of known objects in cluttered scenes from a single RGB image, in near real time and without the need for post-alignment.The DOPE network is a convolutional deep neural network that detects objects' 3D keypoints using multistage architechture. 

Firstly, the image features are extracted by the first ten layers of the VGG-19 convolutional neural network (with pre-trained parameters). Then two 3 × 3 convolutional are applied to the features to reduce the feature dimensions from 512 to 128. 

Second, these 128-dimensional features are fed into the first stage, which consists of three 3 × 3 × 128 convolutional layers and one 1 × 1 × 512 layer, followed by a 1 × 1 × 9 to produce belief maps and and 1 × 1 × 16 to produce vector fields. 

There are 9 believe maps, 8 of them are for the projected vertices of the 3D objects and one for its centroid. Vector fields indicate that the direction from vertices to their corresponding centroids, to construct the bounding boxes of objects after detection.  

Data Generation:  

As more data is required to train a high performance deep network, it can be difficult to gather enough data for training. In addition, unlike 2D labeling, making 3D pose labels manually is much more difficult. DOPE proposed a method to generate data, which allows scientists to gather enough number of data rapidly, and greatly alleviate the workload of labeling manually.

The overall data synthesis strategy is to generate two kinds of dataset: "domain randomized (DR)" and "photorealistic (photo)". The domain randomized data are generated by putting the target object into a virtual environment, which is composed of different distractor objects and a random background. The objects shown in DR images do not necessarily obey physical principles. Photorealistic data are generated by putting target objects into 3D backgrounds with physical constraints. In other words, they are impacted by the effects of gravity and collision.


## Algorithmic Extension
Network Architecture  

One of our algorithmic extensions is that we replaced the original VGG19 feature extractor network with ViT, because we perceive ViT's larger receptive field and its ability to relate the global scene, rather than focus on a local area. To make this change, many parts of the original model backbone need to be modified.We created a pre-trained ViT feature extractor using the timm library. It accepts images of dimension 244 × 244 with a patch size 16 × 16, as a result, interpolating is needed to make sure the input data has a size of 244 × 244. Then we take the output from ViT only in the final layer. At the next stage, two convolutional layers are employed to reduce the number of channel to 128, hence the dimension matches the following network structure (the belief map stages).

Data Generation  

We enhanced the original data generation pipeline using BlenderProc to produce two distinct synthetic RGB datasets, each corresponding to a specific target object: Cookies and Block. The Cookies object is part of the publicly available HOPE dataset, while the Block is a newly introduced, custom-designed object. Our pipeline incorporates randomized camera poses, object poses, and 360-degree HDRI backgrounds, while ensuring that these variations remain physically reasonable. These improvements aim to create a more diverse and robust synthetic dataset, helping to mitigate the common sim-to-real domain gap in deep learning applications. The enhanced pipeline consists of four main stages: (1) textured 3D CAD modeling, (2) real-world HDRI background generation, (3) image synthesis, and (4) ground truth annotation pre-processing.

  (1)Textured 3D CAD Modeling and Real-World Background Generation

  To obtain a precise 3D textured model of the customized object, we first used SolidWorks to create an accurate geometric model with correct dimensions. Blender was then employed to add textures and enrich visual details, including colors and physical material properties, as shown in below.

<div class="center-image">
<img alt="Teaser Figure" src="{{ site.baseurl }}/assets/projects/reports/DOPE-Plus/block_blender.png" />
</div>
<p align="center">3D Textured Model</p>
  For real-world HDRI background generation, we captured raw 360-degree images of the desired physical environments using the Insta360 X3 camera. These images were subsequently pre-processed and converted into HDRI backgrounds using Adobe Photoshop, as illustrated below.
  <div class="center-image">
<img alt="Teaser Figure" src="{{ site.baseurl }}/assets/projects/reports/DOPE-Plus/HDR_example.png" style="max-width: 60%;"/>
</div>
<p align="center">Sampled HDRI Background</p>

  (2)Image Synthesis

  With all necessary elements prepared, we proceeded to the image synthesis stage. We developed a Python script to randomize the poses of cameras, target objects, and distractors. To emulate typical indoor scenarios encountered in onboard SLAM and manipulation tasks, we assumed that both the camera and the target object remained upright, with randomized yaw angles and small perturbations in pitch and roll. In contrast, distractor objects were randomized with full degrees of freedom as a form of data augmentation, without adhering to physical stability constraints.

  (3)Ground Truth Annotation Pre-Processing

  With the existing pipeline provided by original paper, ground truth annotations for each frame were automatically generated. However, when constructing a comprehensive dataset for training and validation, it was necessary to combine synthetic and real images from various sources. In this case, the annotation files (e.g., JSON files) often differed in format and configuration. To streamline data preparation and ensure compatibility with downstream tasks, we developed an additional Python script to pre-process and standardize the ground truth annotations.




## Innovative Enhanced Datasets

We augmented the original HOPE dataset and created a new dataset for the customized Block object by generating synthetic domain-randomized (DR) images, referred to as HOPE-Syn&Real and the Synthetic Block Dataset, respectively.

  (1)HOPE Data Augmentation (HOPE-Syn&Real Dataset)

  We generated additional synthetic data based on the HOPE dataset. The original dataset consists of 28 grocery items, with approximately 300 real images per object. We selected Cookies as the target object for subsequent training tasks. To enrich the existing dataset, we synthesized additional 12,000 domain-randomized (DR) images of this object using the enhanced data generation pipeline developed upon, and combined them with the existing real images to form the HOPE-Syn\&Real dataset. To verify the quality of the synthesized images, we employed a validation method adapted from the original codebase to visualize the ground truth annotations, as shown in below.
  <div style="display: flex; justify-content: center; gap: 1em;">
  <img alt="Teaser Figure" src="{{ site.baseurl }}/assets/projects/reports/DOPE-Plus/000215.png" style="max-width: 13.5%;">
  <img alt="Teaser Figure" src="{{ site.baseurl }}/assets/projects/reports/DOPE-Plus/000215-validate.png" style="max-width: 13.5%;">
  </div> -->

  <div style="display: flex; justify-content: center; gap: 1em;">
  <img alt="Teaser Figure" src="{{ site.baseurl }}/assets/projects/reports/DOPE-Plus/000210.png" style="max-width: 35%;">
  <img alt="Teaser Figure" src="{{ site.baseurl }}/assets/projects/reports/DOPE-Plus/000210-validate.png" style="max-width: 35%;">
  </div> -->
  <p align="center">Sampled Generated Data and Visualized Ground Truth in the HOPE-Syn&Real Dataset. (Left column: generated RGB images, Right column: visualized ground truths)</p>

  (2)Synthetic Block Dataset

  In addition to augmenting the HOPE dataset, we created a fully synthetic dataset for our customized Block object using the aforementioned methods and strategies. This dataset consists of over 19,300 domain-randomized images, with random variations in block poses, instance counts, backgrounds, and distractor objects. Furthermore, as shown below, lighting conditions and shadows were simulated and rendered to further enhance realism and dataset diversity.
  <div class="center-image">
<img alt="Teaser Figure" src="{{ site.baseurl }}/assets/projects/reports/DOPE-Plus/000031.png" />
</div>
<p align="center">Synthetic Domain Randomized Image in the Synthetic Block Dataset</p>


<!-- In our project design, we planned to use 2 dataset to implement our training: HOPE dataset and self-synthesis HOPE dataset(HOPE-synthesis for short).

HOPE is a dataset consists of 28 toy grocery items, which are selected to be more compatible with robot manipulation. To make this dataset compatible with our existing code, we first made necessary format changes to the json file (which indicated the ground truth of corresponding image).  

HOPE-synthetic use the data synthesis method provided by original code base, to generate domain randomized (DR) data using HOPE. We also utilized the validation method provided by original code base to visualize the ground truth of the generated data, so we can make sure the data are correct and ready to use.   

In actual training, to ensure the accuracy, we use hybrid dataset(HOPE mixed with HOPE-synthetic) and pure HOPE-synthetic dataset to train the model separately, since we are not available to photorealistic synthetic method and not having enough data to synthesize them.  

The ground truth of HOPE and HOPE-synthetic are shown below:
<div style="display: flex; justify-content: center; gap: 1em;">
  <img alt="Teaser Figure" src="{{ site.baseurl }}/assets/projects/reports/example/HOPE_GroundTruth.png" style="max-width: 40%;">
  <img alt="Teaser Figure" src="{{ site.baseurl }}/assets/projects/reports/example/HOPE-syntheic_GT.png" style="max-width: 40%;">
</div> -->



## Results

To quantify and compare model performance, we trained four models for Cookies and Block in total: one original DOPE and one ViT-DOPE model for each object. The HOPE-Syn&Real Dataset and the Synthetic Block Dataset were used to train Cookies and Block models, respectively. Each used dataset was split into training and validation subsets, where the validation sets contained around 5% - 7% of the total images. Both datasets do not contain photorealistic images due to project deadline constraints and the lack of open-sourced data generation scripts in the original DOPE codebase. Hence, Cookies' models were trained with DR and real images while Block's models were merely trained on DR data. 

The statistic results of object "Cookies" are shown below:
<div style="display: flex; justify-content: center; gap: 1em; flex-wrap: wrap;">
  <img alt="Loss" src="{{ site.baseurl }}/assets/projects/reports/DOPE-Plus/Loss_cookies.png" style="max-width: 30%; height: auto;">
  <img alt="Accuracy" src="{{ site.baseurl }}/assets/projects/reports/DOPE-Plus/accuracy_cookies.png" style="max-width: 30%; height: auto;">
  <img alt="Accuracy" src="{{ site.baseurl }}/assets/projects/reports/DOPE-Plus/mAP_cookies.png" style="max-width: 30%; height: auto;">
</div>
<p align="center">Loss, Accuracy and mAP Values of "Cookies" Object Training</p>


The statistic results of object "Block" are shown below:
<div style="display: flex; justify-content: center; gap: 1em; flex-wrap: wrap;">
  <img alt="Loss" src="{{ site.baseurl }}/assets/projects/reports/DOPE-Plus/Loss_block.png" style="max-width: 30%; height: auto;">
  <img alt="Accuracy" src="{{ site.baseurl }}/assets/projects/reports/DOPE-Plus/accuracy_block.png" style="max-width: 30%; height: auto;">
  <img alt="Accuracy" src="{{ site.baseurl }}/assets/projects/reports/DOPE-Plus/mAP_block.png" style="max-width: 30%; height: auto;">
</div>
<p align="center">Loss, Accuracy and mAP Values of "Block" Object Training</p>

An example of model belief map prediction is shown below:
<div class="center-image">
<img alt="Teaser Figure" src="{{ site.baseurl }}/assets/projects/reports/DOPE-Plus/000024_belief_maps.png" style="max-width: 30%; height: auto;"/>
</div>
<p align="center">Model Inference Example</p>

An example of our model inference to predict the object's bounding box is shown below:
<div class="center-image">
<img alt="Teaser Figure" src="{{ site.baseurl }}/assets/projects/reports/DOPE-Plus/inference_predict.png" />
</div>
<p align="center">Model Inference Example</p>

<!-- ![DeepRob Logo]({{ site.baseurl }}/assets/logos/favicons/UMich_favicon_dark.png) -->


<!-- ## Project Video

You can display a video with your model's results by either uploading to youtube, then copying your video's `<iframe>` source as shown below. Alternatively if your video files are small, we can host them directly on the DeepRob server.

<div class="video-wrap">
  <div class="video-container">
	<iframe src="https://www.youtube.com/embed/dx1G7y6mhMQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
  </div>
</div> -->


## Citation

If you found our work helpful, consider citing us with the following BibTeX reference:

```
@article{jeffery2025deeprob,
  title = {Feature Extraction Enhancement for 6D Pose Estimation},
  author = {Chen, Jeffery and Luo, Yuqiao and Yuan, Longzhen},
  year = {2025}
}
```



## Contact

If you have any questions, feel free to contact [Jeffery Chen, Yuqiao Luo and Longzhen Yuan](mailto:jeffzc@umich.edu?cc=joeluo@umich.edu?cc=longzhen@umich.edu).

