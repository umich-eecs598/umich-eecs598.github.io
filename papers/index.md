---
layout: page
title: Papers
description: Collection of deep learning research papers with coverage in perception and associated robotic tasks.
nav_order: 5
has_children: false
has_toc: true
---

# Deep Learning Research Papers for Robot Perception
{:.no_toc}

A collection of deep learning research papers with coverage in perception and associated robotic tasks. Within each research area outlined below, the course staff has identified a *core* and *extended* set of research papers. The *core* set of papers will form the basis of our seminar-style lectures starting in [week 8]({{ site.baseurl }}/calendar/#week-8-3d-perception). The *extended* set provides additional coverage of even more exciting work being done within each area.

---

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

# RGB-D Architectures 

[Scheduled Week 8, Lec 14]({{ site.baseurl }}/calendar/#lec-14)

### Core List
{: .no_toc }

1. [PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes](https://arxiv.org/abs/1711.00199){: target="_blank" rel="noopener noreferrer"}, Xiang et al., 2018

2. [A Unified Framework for Multi-View Multi-Class Object Pose Estimation](https://arxiv.org/abs/1803.08103){: target="_blank" rel="noopener noreferrer"}, Li et al., 2018

3. [PVN3D: A Deep Point-Wise 3D Keypoints Voting Network for 6DoF Pose Estimation](https://openaccess.thecvf.com/content_CVPR_2020/papers/He_PVN3D_A_Deep_Point-Wise_3D_Keypoints_Voting_Network_for_6DoF_CVPR_2020_paper.pdf){: target="_blank" rel="noopener noreferrer"}, He et al., 2020

4. [Learning RGB-D Feature Embeddings for Unseen Object Instance Segmentation](https://proceedings.mlr.press/v155/xiang21a/xiang21a.pdf){: target="_blank" rel="noopener noreferrer"}, Li et al., 2021




### Extended List
{: .no_toc }

- [3D ShapeNets: A Deep Representation for Volumetric Shapes](https://arxiv.org/abs/1406.5670){: target="_blank" rel="noopener noreferrer"}, Wu et al., 2015

- [VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition](https://graphics.stanford.edu/courses/cs233-21-spring/ReferencedPapers/voxnet_07353481.pdf){: target="_blank" rel="noopener noreferrer"}, Maturana et al., 2015

- [Multi-view Convolutional Neural Networks for 3D Shape Recognition](https://openaccess.thecvf.com/content_iccv_2015/papers/Su_Multi-View_Convolutional_Neural_ICCV_2015_paper.pdf){: target="_blank" rel="noopener noreferrer"}, Su et al., 2015

- [Volumetric and Multi-View CNNs for Object Classification on 3D Data](https://openaccess.thecvf.com/content_cvpr_2016/papers/Qi_Volumetric_and_Multi-View_CVPR_2016_paper.pdf){: target="_blank" rel="noopener noreferrer"}, Qi et al., 2016

- [Robust 6D Object Pose Estimation with Stochastic Congruent Sets](https://arxiv.org/abs/1805.06324){: target="_blank" rel="noopener noreferrer"}, Mitash et al., 2018

- [What's Behind the Couch? Directed Ray Distance Functions (DRDF) for 3D Scene Reconstruction](https://arxiv.org/abs/2112.04481){: target="_blank" rel="noopener noreferrer"}, Kulkarni et al., 2022


# Point Cloud Processing

[Scheduled Week 8, Lec 15]({{ site.baseurl }}/calendar/#lec-15)

### Core List
{: .no_toc }

1. [PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation](https://arxiv.org/abs/1612.00593){: target="_blank" rel="noopener noreferrer"}, Qi et al., 2017


2. [PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space](https://arxiv.org/abs/1706.02413){: target="_blank" rel="noopener noreferrer"}, Qi et al., 2017


3. [PointFusion: Deep Sensor Fusion for 3D Bounding Box Estimation](https://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_PointFusion_Deep_Sensor_CVPR_2018_paper.pdf){: target="_blank" rel="noopener noreferrer"}, Xu et al., 2018

4. [DenseFusion: 6D Object Pose Estimation by Iterative Dense Fusion](https://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_DenseFusion_6D_Object_Pose_Estimation_by_Iterative_Dense_Fusion_CVPR_2019_paper.pdf){: target="_blank" rel="noopener noreferrer"}, Wang et al., 2019


### Extended List
{: .no_toc }

- [Just Go with the Flow: Self-Supervised Scene Flow Estimation](https://arxiv.org/abs/1912.00497){: target="_blank" rel="noopener noreferrer"}, Mittal et al., 2019

- [PointFlow: 3D Point Cloud Generation with Continuous Normalizing Flows](https://arxiv.org/abs/1906.12320){: target="_blank" rel="noopener noreferrer"}, Yang et al., 2019

- [3D Object Detection with Pointformer](https://openaccess.thecvf.com/content/CVPR2021/papers/Pan_3D_Object_Detection_With_Pointformer_CVPR_2021_paper.pdf){: target="_blank" rel="noopener noreferrer"}, Pan et al., 2021

- [Particle Video Revisited: Tracking Through Occlusions Using Point Trajectories](https://arxiv.org/abs/2204.04153){: target="_blank" rel="noopener noreferrer"}, Harley et al., 2022



# Object Pose, Geometry, SDF, Implicit surfaces

[Scheduled Week 9, Lec 16]({{ site.baseurl }}/calendar/#lec-16)

### Core List
{: .no_toc }

1. [SUM: Sequential scene understanding and manipulation](https://ieeexplore.ieee.org/abstract/document/8206164){: target="_blank" rel="noopener noreferrer"}, Sui et al., 2017

2. [DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation](https://openaccess.thecvf.com/content_CVPR_2019/papers/Park_DeepSDF_Learning_Continuous_Signed_Distance_Functions_for_Shape_Representation_CVPR_2019_paper.pdf){: target="_blank" rel="noopener noreferrer"}, Park et al., 2019

3. [Implicit surface representations as layers in neural networks](https://openaccess.thecvf.com/content_ICCV_2019/papers/Michalkiewicz_Implicit_Surface_Representations_As_Layers_in_Neural_Networks_ICCV_2019_paper.pdf){: target="_blank" rel="noopener noreferrer"}, Michalkiewicz et al., 2019

4. [iSDF: Real-Time Neural Signed Distance Fields for Robot Perception](https://arxiv.org/abs/2204.02296){: target="_blank" rel="noopener noreferrer"}, Oriz et al., 2022


### Extended List
{: .no_toc }

- [Local Deep Implicit Functions for 3D Shape](https://arxiv.org/abs/1912.06126){: target="_blank" rel="noopener noreferrer"}, Genova et al., 2020

- [Implicit geometric regularization for learning shapes](https://arxiv.org/abs/2002.10099){: target="_blank" rel="noopener noreferrer"}, Gropp et al., 2020

- [TAX-Pose: Task-Specific Cross-Pose Estimation for Robot Manipulation](https://arxiv.org/abs/2211.09325){: target="_blank" rel="noopener noreferrer"}, Pan et al., 2022

- [Improving Object Pose Estimation by Fusion With a Multimodal Prior â€“ Utilizing Uncertainty-Based CNN Pipelines for Robotics](https://ieeexplore.ieee.org/document/9670642){: target="_blank" rel="noopener noreferrer"}, Richter-Klug et al., 2022


# Dense Descriptors, Category-level Representations 

[Scheduled Week 9, Lec 17]({{ site.baseurl }}/calendar/#lec-17)

### Core List
{: .no_toc }

1. [Dense Object Nets: Learning Dense Visual Object Descriptors By and For Robotic Manipulation](https://arxiv.org/abs/1806.08756){: target="_blank" rel="noopener noreferrer"}, Florence et al., 2018

2. [Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation](https://geometry.stanford.edu/projects/NOCS_CVPR2019/){: target="_blank" rel="noopener noreferrer"}, Wang et al., 2019

3. [kPAM: KeyPoint Affordances for Category-Level Robotic Manipulation](https://arxiv.org/abs/1903.06684){: target="_blank" rel="noopener noreferrer"}, Manuelli et al., 2019

4. [Single-Stage Keypoint-Based Category-Level Object Pose Estimation from an RGB Image](https://arxiv.org/abs/2109.06161){: target="_blank" rel="noopener noreferrer"}, Lin et al., 2022 


### Extended List
{: .no_toc }

- [Visual Descriptor Learning from Monocular Video](https://arxiv.org/abs/2004.07007){: target="_blank" rel="noopener noreferrer"}, Deekshith et al., 2020

- [SurfEmb: Dense and Continuous Correspondence Distributions for Object Pose Estimation with Learnt Surface Embeddings](https://arxiv.org/abs/2111.13489){: target="_blank" rel="noopener noreferrer"}, Haugaard et al., 2021




# Recurrent Networks and Object Tracking 

[Scheduled Week 10, Lec 18]({{ site.baseurl }}/calendar/#lec-18)

### Core List
{: .no_toc }

1. [DeepIM: Deep Iterative Matching for 6D Pose Estimation](https://openaccess.thecvf.com/content_ECCV_2018/papers/Yi_Li_DeepIM_Deep_Iterative_ECCV_2018_paper.pdf){: target="_blank" rel="noopener noreferrer"}, Li et al., 2018

2. [PoseRBPF: A Rao-Blackwellized Particle Filter for 6D Object Pose Tracking](https://arxiv.org/abs/1905.09304){: target="_blank" rel="noopener noreferrer"}, Deng et al., 2019

3. [6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints](https://ieeexplore.ieee.org/abstract/document/9196679){: target="_blank" rel="noopener noreferrer"}, Wang et al., 2020

4. [XMem: Long-Term Video Object Segmentation with an Atkinson-Shiffrin Memory Model](https://arxiv.org/abs/2207.07115){: target="_blank" rel="noopener noreferrer"}, Cheng and Schwing, 2022


### Extended List
{: .no_toc }

- [Long Short-Term Memory](https://ieeexplore.ieee.org/abstract/document/6795963){: target="_blank" rel="noopener noreferrer"}, Hochreiter et al., 1997

- [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/){: target="_blank" rel="noopener noreferrer"}, Karpathy, 2015

- [TrackFormer: Multi-Object Tracking with Transformers](https://openaccess.thecvf.com/content/CVPR2022/papers/Meinhardt_TrackFormer_Multi-Object_Tracking_With_Transformers_CVPR_2022_paper.pdf){: target="_blank" rel="noopener noreferrer"}, Meinhardt et al., 2022

- [RNNPose: Recurrent 6-DoF Object Pose Refinement with Robust Correspondence Field Estimation and Pose Optimization](https://arxiv.org/abs/2203.12870v3){: target="_blank" rel="noopener noreferrer"}, Xu et al., 2022

# Visual Odometry and Localization 

[Scheduled Week 10, Lec 19]({{ site.baseurl }}/calendar/#lec-19)

### Core List
{: .no_toc }

1. [Backprop KF: Learning Discriminative Deterministic State Estimators](https://proceedings.neurips.cc/paper/2016/file/697e382cfd25b07a3e62275d3ee132b3-Paper.pdf){: target="_blank" rel="noopener noreferrer"}, Haarnoja et al., 2016

2. [Differentiable Particle Filters: End-to-End Learning with Algorithmic Priors](http://www.roboticsproceedings.org/rss14/p01.pdf){: target="_blank" rel="noopener noreferrer"}, Jonschkowski et al., 2018

3. [Multimodal Sensor Fusion with Differentiable Filters](https://arxiv.org/abs/2010.13021){: target="_blank" rel="noopener noreferrer"}, Lee et al., 2020

4. [Differentiable SLAM-net: Learning Particle SLAM for Visual Navigation](https://openaccess.thecvf.com/content/CVPR2021/papers/Karkus_Differentiable_SLAM-Net_Learning_Particle_SLAM_for_Visual_Navigation_CVPR_2021_paper.pdf){: target="_blank" rel="noopener noreferrer"}, Karkus et al., 2021


### Extended List
{: .no_toc }

- [Factor Graphs and GTSAM](https://gtsam.org/tutorials/intro.html){: target="_blank" rel="noopener noreferrer"}, Dellaert et al., 2012

- [SuperPoint: Self-Supervised Interest Point Detection and Description](https://arxiv.org/abs/1712.07629){: target="_blank" rel="noopener noreferrer"}, DeTone et al., 2017

- [Particle Filter Recurrent Neural Networks](https://arxiv.org/abs/1905.12885){: target="_blank" rel="noopener noreferrer"}, Ma et al., 2019

- [Differentiable Algorithm Networks for Composable Robot Learning](https://arxiv.org/pdf/1905.11602.pdf){: target="_blank" rel="noopener noreferrer"}, Karkus et al., 2019

- [SuperGlue: Learning Feature Matching with Graph Neural Networks](https://arxiv.org/abs/1911.11763){: target="_blank" rel="noopener noreferrer"}, Sarlin et al., 2019

- [Chasing Ghosts: Instruction Following as Bayesian State Tracking](https://proceedings.neurips.cc/paper/2019/file/82161242827b703e6acf9c726942a1e4-Paper.pdf){: target="_blank" rel="noopener noreferrer"}, Anderson et al., 2019

- [Differentiable Factor Graph Optimization for Learning Smoothers](https://arxiv.org/abs/2105.08257){: target="_blank" rel="noopener noreferrer"}, Yi et al., 2021

- [How to train your differentiable filter](https://link.springer.com/article/10.1007/s10514-021-09990-9){: target="_blank" rel="noopener noreferrer"}, Kloss et al., 2021

- [Differentiable Nonparametric Belief Propagation](https://arxiv.org/abs/2101.05948){: target="_blank" rel="noopener noreferrer"}, Opipari et al., 2021

- [A Robot Web for Distributed Many-Device Localisation](https://arxiv.org/abs/2202.03314){: target="_blank" rel="noopener noreferrer"}, Murai et al., 2022


# Semantic Scene Graphs and Explicit Representations 

[Scheduled Week 11, Lec 20]({{ site.baseurl }}/calendar/#lec-20)

### Core List
{: .no_toc }

1. [Image Retrieval using Scene Graphs](https://openaccess.thecvf.com/content_cvpr_2015/papers/Johnson_Image_Retrieval_Using_2015_CVPR_paper.pdf){: target="_blank" rel="noopener noreferrer"}, Johnson et al., 2015

2. [Semantic Robot Programming for Goal-Directed Manipulation in Cluttered Scenes](https://ieeexplore.ieee.org/abstract/document/8460538){: target="_blank" rel="noopener noreferrer"}, Zeng et al., 2018

3. [Semantic Linking Maps for Active Visual Object Search](https://arxiv.org/abs/2006.10807){: target="_blank" rel="noopener noreferrer"}, Zeng et al., 2020

4. [Hydra: A Real-time Spatial Perception System for 3D Scene Graph Construction and Optimization](https://arxiv.org/abs/2201.13360){: target="_blank" rel="noopener noreferrer"}, Hughes et al., 2022


### Extended List
{: .no_toc }

- [RoboSherlock: Unstructured information processing for robot perception](https://ieeexplore.ieee.org/abstract/document/7139395){: target="_blank" rel="noopener noreferrer"}, Beetz et al., 2015

- [Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations](https://arxiv.org/abs/1602.07332){: target="_blank" rel="noopener noreferrer"}, Krishna et al., 2016

- [Image Generation from Scene Graphs](https://openaccess.thecvf.com/content_cvpr_2018/papers/Johnson_Image_Generation_From_CVPR_2018_paper.pdf){: target="_blank" rel="noopener noreferrer"}, Johnson et al., 2018

- [3D Scene Graph: A Structure for Unified Semantics, 3D Space, and Camera](https://openaccess.thecvf.com/content_ICCV_2019/html/Armeni_3D_Scene_Graph_A_Structure_for_Unified_Semantics_3D_Space_ICCV_2019_paper.html){: target="_blank" rel="noopener noreferrer"}, Armeni et al., 2020

- [Differentiable Scene Graphs](https://arxiv.org/abs/1902.10200){: target="_blank" rel="noopener noreferrer"}, Raboh et al., 2020

- [ConceptFusion: Open-set Multimodal 3D Mapping](https://arxiv.org/abs/2302.07241){: target="_blank" rel="noopener noreferrer"}, Jatavallabhula et al., 2023


# Neural Radiance Fields and Implicit Representations 

[Scheduled Week 11, Lec 21]({{ site.baseurl }}/calendar/#lec-21)

### Core List
{: .no_toc }

1. [NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis](https://arxiv.org/abs/2003.08934){: target="_blank" rel="noopener noreferrer"}, Mildenhall et al., 2020

2. [iMAP: Implicit Mapping and Positioning in Real-Time](https://arxiv.org/abs/2103.12352){: target="_blank" rel="noopener noreferrer"}, Sucar et al., 2021

3. [NeRF-SLAM: Real-Time Dense Monocular SLAM with Neural Radiance Fields](https://arxiv.org/abs/2210.13641){: target="_blank" rel="noopener noreferrer"}, Rosinol et al., 2022

4. [NARF22: Neural Articulated Radiance Fields for Configuration-Aware Rendering](https://arxiv.org/abs/2210.01166){: target="_blank" rel="noopener noreferrer"}, Lewis et al., 2022

5. [Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation](https://f3rm.csail.mit.edu){: target="_blank" rel="noopener noreferrer"}, Shen et al., 2023


### Extended List
{: .no_toc }

- [NeRF Explosion 2020](https://dellaert.github.io/NeRF/){: target="_blank" rel="noopener noreferrer"}, Dellaert, 2020

- [Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations](https://arxiv.org/abs/1906.01618){: target="_blank" rel="noopener noreferrer"}, Sitzmann et al., 2019

- [Local Implicit Grid Representations for 3D Scenes](https://arxiv.org/abs/2003.08981){: target="_blank" rel="noopener noreferrer"}, Jiang et al., 2020

- [Convolutional occupancy networks](https://arxiv.org/abs/2003.04618){: target="_blank" rel="noopener noreferrer"}, Peng et al., 2020

- [Object-Centric Neural Scene Rendering](https://arxiv.org/abs/2012.08503){: target="_blank" rel="noopener noreferrer"}, Guo et al., 2020

- [INeRF: Inverting Neural Radiance Fields for Pose Estimation](https://arxiv.org/abs/2012.05877){: target="_blank" rel="noopener noreferrer"}, Yen-Chen et al., 2021

- [ILabel: Interactive Neural Scene Labelling](https://arxiv.org/abs/2111.14637){: target="_blank" rel="noopener noreferrer"}, Zhi et al., 2021

- [Neural Descriptor Fields: SE(3)-Equivariant Object Representations for Manipulation](https://arxiv.org/abs/2112.05124){: target="_blank" rel="noopener noreferrer"}, Simeonov et al., 2021

- [BungeeNeRF: Progressive Neural Radiance Field for Extreme Multi-scale Scene Rendering](https://arxiv.org/abs/2112.05504){: target="_blank" rel="noopener noreferrer"}, Xiangli et al., 2021

- [Block-NeRF: Scalable Large Scene Neural View Synthesis](https://arxiv.org/abs/2202.05263){: target="_blank" rel="noopener noreferrer"}, Tancik et al., 2022

- [NeRF-Supervision: Learning Dense Object Descriptors from Neural Radiance Fields](https://yenchenlin.me/nerf-supervision/){: target="_blank" rel="noopener noreferrer"}, Yen-Chen et al., 2022

- [Language Embedded Radiance Fields](https://www.lerf.io){: target="_blank" rel="noopener noreferrer"}, Kerr et al., 2023


# Datasets 

[Scheduled Week 12, Lec 22]({{ site.baseurl }}/calendar/#lec-22)

### Core List
{: .no_toc }

1. [Deep Learning for Robots: Learning from Large-Scale Interaction](https://ai.googleblog.com/2016/03/deep-learning-for-robots-learning-from.html){: target="_blank" rel="noopener noreferrer"}, Levine et al., 2016

2. [Isaac Gym: High Performance GPU-Based Physics Simulation For Robot Learning](https://arxiv.org/abs/2108.10470){: target="_blank" rel="noopener noreferrer"}, Makoviychuk et al., 2021

3. [Grounding Predicates through Actions](https://arxiv.org/abs/2109.14718){: target="_blank" rel="noopener noreferrer"}, Migimatsu and Bohg, 2022 

4. [All You Need is LUV: Unsupervised Collection of Labeled Images using Invisible UV Fluorescent Indicators](https://arxiv.org/abs/2203.04566){: target="_blank" rel="noopener noreferrer"}, Thananjeyan et al., 2022



### Extended List
{: .no_toc }

#### Collecting data with robots
{: .no_toc }

- [TossingBot: Learning to Throw Arbitrary Objects](https://tossingbot.cs.princeton.edu/){: target="_blank" rel="noopener noreferrer"}, Zeng et al., 2019


#### RGB-D Datasets
{: .no_toc }

- [(NYU Depth v2) Indoor Segmentation and Support Inference from RGBD Images](https://cs.nyu.edu/~silberman/papers/indoor_seg_support.pdf){: target="_blank" rel="noopener noreferrer"}, Silberman et al., 2012

- [SUN RGB-D: A RGB-D Scene Understanding Benchmark Suite](https://openaccess.thecvf.com/content_cvpr_2015/papers/Song_SUN_RGB-D_A_2015_CVPR_paper.pdf){: target="_blank" rel="noopener noreferrer"}, Song et al., 2015

- [YCB-Video Dataset](https://arxiv.org/abs/1711.00199){: target="_blank" rel="noopener noreferrer"}, Xiang et al., 2018

- [BOP: Benchmark for 6D Object Pose Estimation](https://bop.felk.cvut.cz/home/){: target="_blank" rel="noopener noreferrer"}, HodaÅˆ et al., 2019

- [ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes](http://www.scan-net.org){: target="_blank" rel="noopener noreferrer"}, Dai et al., 2019

- [ProgressLabeller: Visual Data Stream Annotation for Training Object-Centric 3D Perception](https://arxiv.org/abs/2203.00283){: target="_blank" rel="noopener noreferrer"}, Chen et al., 2022

- [TO-Scene: A Large-scale Dataset for Understanding 3D Tabletop Scenes](https://arxiv.org/abs/2203.09440){: target="_blank" rel="noopener noreferrer"}, Xu et al., 2022


#### Semantic Datasets
{: .no_toc }

- [Understanding Human Hands in Contact at Internet Scale](https://arxiv.org/abs/2006.06669){: target="_blank" rel="noopener noreferrer"}, Shan et al., 2020

- [Habitat-Matterport 3D Semantics Dataset](https://arxiv.org/abs/2210.05633){: target="_blank" rel="noopener noreferrer"}, Yadav et al., 2022 


#### Object Model Datasets
{: .no_toc }

- [ShapeNet: An Information-Rich 3D Model Repository](https://shapenet.org){: target="_blank" rel="noopener noreferrer"}, Chang et al., 2015

- [PartNet-Mobility Dataset](https://sapien.ucsd.edu/browse){: target="_blank" rel="noopener noreferrer"}


#### Simulators
{: .no_toc }

- [MuJoCo: A physics engine for model-based control](https://ieeexplore.ieee.org/abstract/document/6386109){: target="_blank" rel="noopener noreferrer"}, Todorov et al., 2015

- [Pybullet, a python module for physics simulation for games, robotics and machine learning](https://pybullet.org/wordpress/){: target="_blank" rel="noopener noreferrer"}, Coumans et al., 2015

- [NVIDIA Isaac Sim](https://developer.nvidia.com/isaac-sim){: target="_blank" rel="noopener noreferrer"}

- [CARLA: An Open Urban Driving Simulator](https://carla.org){: target="_blank" rel="noopener noreferrer"}, Dosovitskiy et al., 2017

- [SoftGym: Benchmarking Deep Reinforcement Learning for Deformable Object Manipulation](https://arxiv.org/abs/2011.07215){: target="_blank" rel="noopener noreferrer"}, Lin et al., 2020

- [ManiSkill2: A Unified Benchmark for Generalizable Manipulation Skills](https://arxiv.org/abs/2302.04659){: target="_blank" rel="noopener noreferrer"}, Gu et al., 2023


# Self-Supervised Learning 

[Scheduled Week 12, Lec 23]({{ site.baseurl }}/calendar/#lec-23)

### Core List
{: .no_toc }

1. [Making Sense of Vision and Touch: Self-Supervised Learning of Multimodal Representations for Contact-Rich Tasks](https://arxiv.org/abs/1810.10191){: target="_blank" rel="noopener noreferrer"}, Lee et al., 2019

2. [VICRegL: Self-Supervised Learning of Local Visual Features](https://arxiv.org/abs/2210.01571){: target="_blank" rel="noopener noreferrer"}, Bardes et al., 2022

3. [Fully Self-Supervised Class Awareness in Dense Object Descriptors](https://proceedings.mlr.press/v164/hadjivelichkov22a.html){: target="_blank" rel="noopener noreferrer"}, Hadjivelichkov and Kanoulas, 2022

4. [Self-Supervised Geometric Correspondence for Category-Level 6D Object Pose Estimation in the Wild](https://kywind.github.io/self-pose){: target="_blank" rel="noopener noreferrer"}, Zhang et al., 2022


### Extended List
{: .no_toc }

- [Emerging Properties in Self-Supervised Vision Transformers](https://arxiv.org/abs/2104.14294){: target="_blank" rel="noopener noreferrer"}, Caron et al., 2021

- [DINOv2: Learning Robust Visual Features without Supervision](https://arxiv.org/abs/2304.07193){: target="_blank" rel="noopener noreferrer"}, Oquab et al., 2023



# Grasp Pose Detection 

[Scheduled Week 13, Lec 24]({{ site.baseurl }}/calendar/#lec-24)

### Core List
{: .no_toc }

1. [Real-Time Grasp Detection Using Convolutional Neural Networks](https://arxiv.org/abs/1412.3128){: target="_blank" rel="noopener noreferrer"}, Redmon and Angelova, 2015

2. [Using Geometry to Detect Grasps in 3D Point Clouds](https://arxiv.org/abs/1501.03100){: target="_blank" rel="noopener noreferrer"}, ten Pas and Platt, 2015

3. [Dex-Net 2.0: Deep Learning to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp Metrics](https://arxiv.org/abs/1703.09312){: target="_blank" rel="noopener noreferrer"}, Mahler et al., 2017

4. [Contact-GraspNet: Efficient 6-DoF Grasp Generation in Cluttered Scenes](https://ieeexplore.ieee.org/abstract/document/9561877){: target="_blank" rel="noopener noreferrer"}, Sundermeyer et al., 2021

5. [Sample Efficient Grasp Learning Using Equivariant Models](https://arxiv.org/abs/2202.09468){: target="_blank" rel="noopener noreferrer"}, Zhu et al., 2022


### Extended List
{: .no_toc }

- [Deep Learning for Detecting Robotic Grasps](https://arxiv.org/abs/1301.3592){: target="_blank" rel="noopener noreferrer"}, Lenz et al., 2013

- [High precision grasp pose detection in dense clutter](https://ieeexplore.ieee.org/abstract/document/7759114){: target="_blank" rel="noopener noreferrer"}, Gualtieri et al., 2016

- [GlassLoc: Plenoptic Grasp Pose Detection in Transparent Clutter](https://ieeexplore.ieee.org/abstract/document/8967685){: target="_blank" rel="noopener noreferrer"}, Zhou et al., 2019

- [MetaGraspNet_v0: A Large-Scale Benchmark Dataset for Vision-driven Robotic Grasping via Physics-based Metaverse Synthesis](https://arxiv.org/abs/2112.14663){: target="_blank" rel="noopener noreferrer"}, Chen et al., 2021

- [Grasp Learning: Models, Methods, and Performance](https://arxiv.org/abs/2211.04895){: target="_blank" rel="noopener noreferrer"}, Platt, 2022


# Tactile Perception for Grasping and Manipulation 

[Scheduled Week 13, Lec 25]({{ site.baseurl }}/calendar/#lec-25)

### Core List
{: .no_toc }

1. [More Than a Feeling: Learning to Grasp and Regrasp using Vision and Touch](https://arxiv.org/abs/1805.11085){: target="_blank" rel="noopener noreferrer"}, Calandra et al., 2018 

2. [Tactile Object Pose Estimation from the First Touch with Geometric Contact Rendering](https://arxiv.org/abs/2012.05205){: target="_blank" rel="noopener noreferrer"}, Bauza et al., 2020

3. [Visuotactile Affordances for Cloth Manipulation with Local Control](https://openreview.net/pdf?id=s6NEzqZKaP-){: target="_blank" rel="noopener noreferrer"}, Sunil et al., 2022

4. [ShapeMap 3-D: Efficient shape mapping through dense touch and vision](https://arxiv.org/abs/2109.09884){: target="_blank" rel="noopener noreferrer"}, Suresh et al., 2022


### Extended List
{: .no_toc }

- [The Feeling of Success: Does Touch Sensing Help Predict Grasp Outcomes?](https://arxiv.org/pdf/1710.05512.pdf){: target="_blank" rel="noopener noreferrer"}, Calandra et al., 2017

- [GelSight: High-Resolution Robot Tactile Sensors for Estimating Geometry and Force](https://dspace.mit.edu/handle/1721.1/114627){: target="_blank" rel="noopener noreferrer"}, Yuan et al., 2017

- [Soft-bubble: A highly compliant dense geometry tactile sensor for robot manipulation](https://arxiv.org/abs/1904.02252){: target="_blank" rel="noopener noreferrer"}, Alspach et al., 2019

- [A Review of Tactile Information: Perception and Action Through Touch](https://ieeexplore.ieee.org/document/9136877){: target="_blank" rel="noopener noreferrer"}, Li et al., 2020

- [TACTO: A Fast, Flexible, and Open-source Simulator for High-Resolution Vision-based Tactile Sensors](https://arxiv.org/pdf/2012.08456.pdf){: target="_blank" rel="noopener noreferrer"}, Wang et al., 2020

- [Active Extrinsic Contact Sensing: Application to General Peg-in-Hole Insertion](https://arxiv.org/abs/2110.03555){: target="_blank" rel="noopener noreferrer"}, Kim et al., 2021

- [Active Visuo-Haptic Object Shape Completion](https://ieeexplore.ieee.org/document/9720238){: target="_blank" rel="noopener noreferrer"}, Rustler et al., 2022

- [Learning Self-Supervised Representations from Vision and Touch for Active Sliding Perception of Deformable Surfaces](https://arxiv.org/abs/2209.13042){: target="_blank" rel="noopener noreferrer"}, Kerr and Huang et al., 2022

- [See, Hear, and Feel: Smart Sensory Fusion for Robotic Manipulation](https://arxiv.org/pdf/2212.03858.pdf){: target="_blank" rel="noopener noreferrer"}, Li et al., 2022

- [Learning to Grasp the Ungraspable with Emergent Extrinsic Dexterity](https://arxiv.org/abs/2211.01500){: target="_blank" rel="noopener noreferrer"}, Zhou and Held, 2022


# Pre-training for Robot Manipulation 

[Scheduled Week 14, Lec 26]({{ site.baseurl }}/calendar/#lec-26)

### Core List
{: .no_toc }

1. [SORNet: Spatial Object-Centric Representations for Sequential Manipulation](https://arxiv.org/abs/2109.03891){: target="_blank" rel="noopener noreferrer"}, Yuan et al., 2021

2. [CLIPort: What and Where Pathways for Robotic Manipulation](https://arxiv.org/abs/2109.12098){: target="_blank" rel="noopener noreferrer"}, Shridhar et al., 2021

3. [Real-World Robot Learning with Masked Visual Pre-training](https://arxiv.org/abs/2210.03109){: target="_blank" rel="noopener noreferrer"}, Radosavovic et al., 2022

4. [R3M: A Universal Visual Representation for Robot Manipulation](https://arxiv.org/abs/2203.12601){: target="_blank" rel="noopener noreferrer"}, Nair et al., 2022

5. [Do As I Can, Not As I Say: Grounding Language in Robotic Affordances](https://say-can.github.io/assets/palm_saycan.pdf){: target="_blank" rel="noopener noreferrer"}, Ahn et al., 2022

6. [RT-1: Robotics Transformer for Real-World Control at Scale](https://robotics-transformer.github.io/assets/rt1.pdf){: target="_blank" rel="noopener noreferrer"}, Brohan et al., 2022


### Extended List
{: .no_toc }

- [Attention and Augmented Recurrent Neural Networks](https://distill.pub/2016/augmented-rnns/){: target="_blank" rel="noopener noreferrer"}, Olah & Carter, 2016

- [Attention is All You Need](https://arxiv.org/abs/1706.03762){: target="_blank" rel="noopener noreferrer"}, Vaswani et al., 2017

- [Feature-wise transformations](https://distill.pub/2018/feature-wise-transformations/){: target="_blank" rel="noopener noreferrer"}, Dumoulin et al., 2018

- [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929){: target="_blank" rel="noopener noreferrer"}, Dosovitskiy et al., 2020

- [Transporter Networks: Rearranging the Visual World for Robotic Manipulation](https://arxiv.org/abs/2010.14406){: target="_blank" rel="noopener noreferrer"}, Zeng et al., 2020

- [CLIP: Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020){: target="_blank" rel="noopener noreferrer"}, Radford et al., 2021

- [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377){: target="_blank" rel="noopener noreferrer"}, He et al., 2021

- [Interactive Language: Talking to Robots in Real Time](https://arxiv.org/abs/2210.06407){: target="_blank" rel="noopener noreferrer"}, Lynch et al., 2022

- [Transformers are Adaptable Task Planners](https://arxiv.org/abs/2207.02442){: target="_blank" rel="noopener noreferrer"}, Jain et al., 2022



# Perception Beyond Vision

### Specialized Sensors
{: .no_toc }

- [Pigeons (Columba livia) as Trainable Observers of Pathology and Radiology Breast Cancer Images](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0141357){: target="_blank" rel="noopener noreferrer"}, Levenson et al., 2015

- [Automatic color correction for 3D reconstruction of underwater scenes](https://ieeexplore.ieee.org/abstract/document/7989601){: target="_blank" rel="noopener noreferrer"}, Skinner et al., 2017

- [GelSight: High-Resolution Robot Tactile Sensors for Estimating Geometry and Force](https://www.mdpi.com/1424-8220/17/12/2762){: target="_blank" rel="noopener noreferrer"}, Yuan et al., 2017

- [Classification of Household Materials via Spectroscopy](https://arxiv.org/abs/1805.04051){: target="_blank" rel="noopener noreferrer"}, Erickson et al., 2018

- [Through-Wall Human Pose Estimation Using Radio Signals](https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhao_Through-Wall_Human_Pose_CVPR_2018_paper.pdf){: target="_blank" rel="noopener noreferrer"}, Zhao et al., 2018

- [A bio-hybrid odor-guided autonomous palm-sized air vehicle](https://iopscience.iop.org/article/10.1088/1748-3190/abbd81){: target="_blank" rel="noopener noreferrer"}, Anderson et al., 2020

- [Event-based, Direct Camera Tracking from a Photometric 3D Map using Nonlinear Optimization](https://rpg.ifi.uzh.ch/docs/ICRA19_Bryner.pdf){: target="_blank" rel="noopener noreferrer"}, Bryner et al., 2019

- [SoundSpaces: Audio-Visual Navigation in 3D Environments](https://arxiv.org/abs/1912.11474){: target="_blank" rel="noopener noreferrer"}, Chen et al., 2019

- [Neural Implicit Surface Reconstruction using Imaging Sonar](https://arxiv.org/abs/2209.08221){: target="_blank" rel="noopener noreferrer"}, Qadri et al., 2022




# More Frontiers 

[Scheduled Week 14, Lec 27]({{ site.baseurl }}/calendar/#lec-27)

### Interpreting Deep Learning Models

- [Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps](https://arxiv.org/abs/1312.6034){: target="_blank" rel="noopener noreferrer"}, Simonyan et al., 2013

- [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://arxiv.org/abs/1610.02391){: target="_blank" rel="noopener noreferrer"}, Selvaraju et al., 2016

- [The Building Blocks of Interpretability](https://distill.pub/2018/building-blocks/){: target="_blank" rel="noopener noreferrer"}, Olah et al., 2018

- [Multimodal Neurons in Artificial Neural Networks](https://distill.pub/2021/multimodal-neurons/){: target="_blank" rel="noopener noreferrer"}, Goh et al., 2021


### Fairness and Ethics

- [Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification](https://proceedings.mlr.press/v81/buolamwini18a.html?mod=article_inline){: target="_blank" rel="noopener noreferrer"}, Buolamwini and Gebru, 2018

- [Saving Face: Investigating the Ethical Concerns of Facial Recognition Auditing](https://dl.acm.org/doi/abs/10.1145/3375627.3375820){: target="_blank" rel="noopener noreferrer"}, Raji et al., 2020


### Certifiable Perception

- [Certifiably Optimal Outlier-Robust Geometric Perception: Semidefinite Relaxations and Scalable Global Optimization](https://arxiv.org/abs/2109.03349){: target="_blank" rel="noopener noreferrer"}, Yang and Carlone, 2021


- [Certifiable 3D Object Pose Estimation: Foundations, Learning Models, and Self-Training](https://arxiv.org/abs/2206.11215){: target="_blank" rel="noopener noreferrer"}, Talak et al., 2022


### Articulated Objects

- [Autonomous Tool Construction Using Part Shape and Attachment Prediction](http://www.roboticsproceedings.org/rss15/p09.pdf){: target="_blank" rel="noopener noreferrer"}, Nair et al., 2019

- [Parts-Based Articulated Object Localization in Clutter Using Belief Propagation](https://arxiv.org/abs/2008.02881){: target="_blank" rel="noopener noreferrer"}, Pavlasek et al., 2020

- [Category-Level Articulated Object Pose Estimation](https://openaccess.thecvf.com/content_CVPR_2020/html/Li_Category-Level_Articulated_Object_Pose_Estimation_CVPR_2020_paper.html){: target="_blank" rel="noopener noreferrer"}, Li et al., 2020

- [Differentiable Nonparametric Belief Propagation](https://arxiv.org/abs/2101.05948){: target="_blank" rel="noopener noreferrer"}, Opipari et al., 2021

- [Category-Independent Articulated Object Tracking with Factor Graphs](https://arxiv.org/abs/2205.03721){: target="_blank" rel="noopener noreferrer"}, Heppert et al., 2022

- [Kineverse: A Symbolic Articulation Model Framework for Model-Agnostic Mobile Manipulation](https://arxiv.org/abs/2012.05362){: target="_blank" rel="noopener noreferrer"}, RÃ¶fer et al., 2022


### Deformable Objects

- [DensePose: Dense Human Pose Estimation In The Wild](https://arxiv.org/abs/1802.00434){: target="_blank" rel="noopener noreferrer"}, Xiao et al., 2018

- [FabricFlowNet: Bimanual Cloth Manipulation with a Flow-based Policy](https://arxiv.org/abs/2111.05623){: target="_blank" rel="noopener noreferrer"}, Weng et al., 2021

- [DextAIRity: Deformable Manipulation Can be a Breeze](https://arxiv.org/abs/2203.01197){: target="_blank" rel="noopener noreferrer"}, Xu et al., 2022

- [Self-supervised Transparent Liquid Segmentation for Robotic Pouring](https://arxiv.org/abs/2203.01538){: target="_blank" rel="noopener noreferrer"}, Narasimhan et al., 2022

- [Visio-tactile Implicit Representations of Deformable Objects](https://arxiv.org/abs/2202.00868){: target="_blank" rel="noopener noreferrer"}, Wi et al., 2022


### Transparent Objects

- [LIT: Light-field Inference of Transparency for Refractive Object Localization](https://arxiv.org/abs/1910.00721){: target="_blank" rel="noopener noreferrer"}, Zhou et al., 2019

- [Multi-modal Transfer Learning for Grasping Transparent and Specular Objects](https://arxiv.org/abs/2006.00028){: target="_blank" rel="noopener noreferrer"}, Weng et al., 2020

- [Dex-NeRF: Using a Neural Radiance Field to Grasp Transparent Objects](https://arxiv.org/abs/2110.14217){: target="_blank" rel="noopener noreferrer"}, Ichnowski et al., 2021

- [ClearPose: Large-scale Transparent Object Dataset and Benchmark](https://arxiv.org/abs/2203.03890){: target="_blank" rel="noopener noreferrer"}, Chen et al., 2022

- [TransNet: Category-Level Transparent Object Pose Estimation](https://arxiv.org/abs/2208.10002){: target="_blank" rel="noopener noreferrer"}, Zhang et al., 2022


### Dynamic Scenes

- [D-NeRF: Neural Radiance Fields for Dynamic Scenes](https://arxiv.org/abs/2011.13961){: target="_blank" rel="noopener noreferrer"}, Pumarola et al., 2020

- [3D Neural Scene Representations for Visuomotor Control](https://arxiv.org/abs/2107.04004){: target="_blank" rel="noopener noreferrer"}, Li et al., 2021

- [HexPlane: A Fast Representation for Dynamic Scenes](https://arxiv.org/abs/2301.09632){: target="_blank" rel="noopener noreferrer"}, Cao and Johnson, 2023



### Beyond 2D Convolutions

- [Learning Decentralized Controllers for Robot Swarms with Graph Neural Networks](https://arxiv.org/abs/1903.10527){: target="_blank" rel="noopener noreferrer"}, Tolstaya et al., 2019

- [A Gentle Introduction to Graph Neural Networks](https://distill.pub/2021/gnn-intro/){: target="_blank" rel="noopener noreferrer"}, Sanchez-Lengeling et al., 2021


### Reinforcement Learning

- [Deep Reinforcement Learning from Human Preferences](https://proceedings.neurips.cc/paper_files/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf){: target="_blank" rel="noopener noreferrer"}, Christiano et al., 2017

- [Understanding RL Vision](https://distill.pub/2020/understanding-rl-vision/){: target="_blank" rel="noopener noreferrer"}, Hilton et al., 2020


### Generative Modeling

- [WaterGAN: Unsupervised Generative Network to Enable Real-time Color Correction of Monocular Underwater Images](https://arxiv.org/abs/1702.07392){: target="_blank" rel="noopener noreferrer"}, Li et al., 2017

- [Differentiable Particle Filters through Conditional Normalizing Flow](https://arxiv.org/abs/2107.00488){: target="_blank" rel="noopener noreferrer"}, Chen et al., 2021

- [Planning with Diffusion for Flexible Behavior Synthesis](https://arxiv.org/abs/2205.09991){: target="_blank" rel="noopener noreferrer"}, Janner et al., 2022

- [Anything-3D: Towards Single-view Anything Reconstruction in the Wild](https://arxiv.org/abs/2304.10261){: target="_blank" rel="noopener noreferrer"}, Shen et al., 2023


